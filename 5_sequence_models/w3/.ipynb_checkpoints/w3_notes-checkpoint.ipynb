{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# WEEK 3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Basic Models\n",
    "\n",
    "* **encoder-decoder architecture**: An encoder embeds a given input into a latent space which is fed to an RNN to \"decode\" it into a sequence. Examples:\n",
    "  1. Sequence-to-sequence: automatic translation. Feed input sequence (french) into an encoder RNN, and then feed the latest hidden state as input to the decoder. The decoder sends its output at $t$ to the $t+1$ input to generate the translated sentence.\n",
    "  2. Image captioning: explaining what's in an image. Similar to 1, but the hidden state is achieved by feeding an image to an RNN and removing, f.e., the last classifier network so we have the highest embedded representation as input to the decoder."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Picking the Most Likely Sentence\n",
    "\n",
    "Instead of sampling as we did before with the \"language\" mdoel, here we are interested in the best possible outcome. The formalization is as follows:\n",
    "\n",
    "### Language model:\n",
    "\n",
    "Before we sampled from a trained RNN to obtain feasible sentences by inputting a zero vector and then sampling from the RNN's  \n",
    "$P(y^{<1>}, ..., y^{<T_y>})$\n",
    "\n",
    "Now we have an extra condition: we don't want random feasible sentences but the best translation for our input sentence:  \n",
    "$P(y^{<1>}, ..., y^{<T_y>} | x^{<1>}, ..., x^{<T_x>})$  \n",
    "Which can be seen as a **conditional language model**\n",
    "\n",
    "The optimization objective is then:\n",
    "\n",
    "\\begin{equation}\n",
    "\\begin{aligned}\n",
    "argmax &= P(y^{<1>}, ..., y^{<T_y>} | x)\\\\\n",
    "y^{<1>}, ..., y^{<T_y>} &\n",
    "\\end{aligned}\n",
    "\\end{equation}\n",
    "\n",
    "In other words, \"pick the $y$ sequence of english words that maximizes the likelihood given the $x$ sequence of french words. Note that there is NOT independence among the words in a sentence:\n",
    "$P(y^{<1>}, ..., y^{<T_y>}) \\neq \\prod_{t=1}^{T_y} P(y^{<t>})$\n",
    "\n",
    "And therefore a greedy search picking the best candidate at each stage will not necessarily provide the best joint distribution. Also note that checking all the combinations will require $|V|^{T_y}$ computations which is in most cases unfeasible."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Beam Search:\n",
    "\n",
    "Instead of searching the  $|V|^{T_y}$ space, set the hyperparameter $B$ as the total number of combinations to be stored. Then greedily increase $t$: for each of our current $B$-best combinations, check through the whole vocabulary for the best next word (by sending the combination so far plus the extra word through the RNN), and then collect the $b$-best outcomes, which will replace the existing ones.\n",
    "\n",
    "Therefore we have $T_y*B*|V|$ computations, and for low B we already retain good chances of finding a close-to-optimal solution.\n",
    "\n",
    "Formalization remember:\n",
    "\n",
    "$P(y^{<1>}, y^{<2>} | x) = P(y^{<1>} | x) P(y^{<2>} | x, y^{<1>})$\n",
    "\n",
    "Note that for $B=1$, we have the naive approach discussed before, where uncorrelated words are assumed.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Refinements to Beam Search:\n",
    "\n",
    "## Log normalization:\n",
    "\n",
    "As in many similar problems, for numerical stability we reformulate the following:\n",
    "\n",
    "\\begin{equation}\n",
    "\\begin{aligned}\n",
    "arg&max \\prod_{t=1}^{T_y} P(y^{<t>} | x, y^{<1>}, ..., y^{<t-1>})\\\\\n",
    "y &\n",
    "\\end{aligned}\n",
    "\\end{equation}\n",
    "\n",
    "As the following with identical outcome\n",
    "\n",
    "\\begin{equation}\n",
    "\\begin{aligned}\n",
    "arg&max \\sum_{t=1}^{T_y} log P(y^{<t>} | x, y^{<1>}, ..., y^{<t-1>})\\\\\n",
    "y &\n",
    "\\end{aligned}\n",
    "\\end{equation}\n",
    "\n",
    "## Length normalization:\n",
    "    \n",
    "We also observe that this system naturally penalizes longer sequences, as they either have same or worse score than shorter ones. Introducing a normalizing factor that depends on the length helps overcome this issue:\n",
    "\n",
    "\\begin{equation}\n",
    "\\begin{aligned}\n",
    "\\frac{1}{T_y^\\alpha} \\sum_{t=1}^{T_y} log P(y^{<t>} | x, y^{<1>}, ..., y^{<t-1>})\\\\\n",
    "\\end{aligned}\n",
    "\\end{equation}\n",
    "\n",
    "The $\\alpha$ soft variable sets a compromise between no normalization at all for $\\alpha=0$ and normalization proportional to the sentence length for $\\alpha=1$. This is a hyperparameter to tune. Empirically it has been shown that values around $\\alpha=0.7$ show good results.\n",
    "\n",
    "The process is then to run the entire beam search up to a fixed length, and keep a record of the B-best sentences at each step. Finally normalize them against this formula depending on their length and pick the best one.\n",
    "\n",
    "Note that a better B is always positive in terms of quality, but it increases the computational costs, and the gain of increasing B decreases greatly with the size of b. For production, 3, 10 or even 100 can be seen. For research, where the models are squeezed, 1000 or even 3000 can be seen.\n",
    "\n",
    "Note that unlike BFS and other search algos this doesnt guarantee to find the global optimum."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Error Analysis in Beam Search:\n",
    "\n",
    "* The task of beam search is to maximize the joint probabilities as returned by the RNN decoder. If the pipeline is not performing well, it can be the RNN's or the BSs fault. Increasing B if it is RNN's fault is basically a waste of time.\n",
    "\n",
    "* To find out, we need a curated translation. Send it through the RNN, and then send the translation accepted by beam search. If, for this mispredicted translation, the probability of the curated one is bigger, we know that beam search messed up. If OTOH the RNN outputs bigger probability for the mistranslated than for the curated, we know that the RNN has to be improved.\n",
    "\n",
    "* Do this for many mistranslated examples, and if beam search is failing in many, it may be a good idea to increase B"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Bleu Score\n",
    "\n",
    "*Papineni et al 2002: A method for automatic evaluation of machine translation*\n",
    "\n",
    "\"Bilingual Evaluation Understudy\": understudy is an agent (person) that can take over when the tenure is away.\n",
    "\n",
    "* Problematic: several sentences could be equally good/close to optimum. This metric is far from perfect but had and still has a deep impact in the NLP community.\n",
    "\n",
    "Setup: we have a single translation but several targets:\n",
    "* Target 1: The cat is on the mat\n",
    "* Target 2: There is a cat on the mat\n",
    "* Prediction: The cat the cat on the mat.\n",
    "\n",
    "Idea: given any set of $n$ subsequent words (n-grams: unigrams, bigrams, trigrams etc...) in the predicted phrase,\n",
    "1. make a histogram of the unique n-grams (i.e. repeated n-grams have one key but higher value): `hist = {k: v+1 for k in n_grams}`\n",
    "2. Clip each count to the maximum number of times that the n-gram appears in either target `clipped = {k:max(count(find(k, target1), count(find(k, target2)) etc) for k,v in hist.items()}`. For example, \"the cat\" appears 2 times in the hist, but only 1 per sentence so the 2 is clipped down to 1\n",
    "3. Divide the sum of the clipped values by the sum of the hist values\n",
    "\n",
    "$\n",
    "p_n = \\frac{\\sum_{n-grams \\in \\hat y} clip_{n-gram}}{\\sum_{n-grams \\in \\hat y} hist_{n-gram}}\n",
    "$\n",
    "\n",
    "If the prediction is identical to any target, the score will be one. This helps as it brings a single, scalar score for this problem.\n",
    "\n",
    "\n",
    "## Combined bleu score:\n",
    "\n",
    "Usually, the score is taken for several sizes and then is collapsed as follows:\n",
    "\n",
    "$\n",
    "BP * exp(average([p_1, p_2, p_3, ...]))\n",
    "$\n",
    "\n",
    "Where BP is a brevity penalty, since shorter sentences will more likely be correct than longer ones. BP is 1 if the output length is bigger than some threshold, and otherwise\n",
    "\n",
    "$\n",
    "exp(1- \\frac{threshold}{reference})\n",
    "$\n",
    "\n",
    "Where reference is some other threshold. Basically an exponential decay model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Attention Model Intuition\n",
    "\n",
    "When the length of the sequence increases, the bleu score decreases (even with brevity penalty): this reflects how humans also translate: they dont memorize all at once and then translate, but go by smaller pieces.\n",
    "\n",
    "The attention model (Bahdanau et al 2014 *Neural machine translation by jointly learning to align and translate*) allows to keep top score for arbitrarily long sequences.\n",
    "\n",
    "## Setup:\n",
    "\n",
    "* Lets pick a BRNN (more often LSTM than GRU, and maybe more than 1 layer) and feed the input text to it, to generate an encoding at each step of the sequence.\n",
    "\n",
    "* Now each encoding is enriched by linearly combining it with the surrounding encodings to create a compromise between a local and rich representation: if we just picked the hidden state after a very long sentence, likely many of the details are lost on the way.\n",
    "\n",
    "* The linear combination (weighted sum) is regulated by a set of $\\alpha$ weights that determine the \"attention span\" at a given point. Ideally, the weights allow all the needed encodings for a short-term translation to pass through, and no more. Since each point relates to all other points, there is potentially a quadratic number of weights to be learned on the top of the BRNN.\n",
    "\n",
    "* On the top of the model is the decoder RNN. Instead of performing a vanilla beam search given the encoded summary as an initial step, now the linearly combined encodings for each step are added, step by step, together with the previous predictions.\n",
    "\n",
    "This is an intuition only, more details in the next video"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Attention Model\n",
    "\n",
    "More details:\n",
    "\n",
    "* The set of $\\alpha$ weights for a given enriched encoding are all non-negative and add up to 1. Note that the encoding at every different input word has a different set of $\\alpha$ weights that have to be determined (quadratic runtime, but ok for relatively short sentences)\n",
    "\n",
    "* The enriched encoding is then a weighted sum $\\sum \\alpha_i concat(a_{forward}, a_{backward})$ between each alpha and the concatenation of the forward and backward hidden $a$ state of the BRNN.\n",
    "\n",
    "Note that we still don't know how to determine the $\\alpha$ weights or how does exactly the decoder receive its last hidden state and new enriched encoding as input. This is both addressed as follows:\n",
    "\n",
    "1. First, concatenate the last hidden state of the decoder with the current non-enriched output of the encoder, and send them through a simple NN. It has to be small since it is computed many times. The output of that NN will have same dimensionality as the $\\alpha$ vector of weights, and will basically determine the attention to the context: \"It seems pretty natural that it depends on those 2 inputs, but we don't know the function so training a NN seems a fair approach\".\n",
    "\n",
    "2. Send the output of the NN through a softmax function: The result is exactly the definition for the $\\alpha$ weights at the current point.\n",
    "\n",
    "3. So the current enriched encoding is the weighted sum between the current $\\alpha$ vector and the corresponding encodings. This is fed to the decoder, concatenated with the last predicted word.\n",
    "\n",
    "4. The decoder will update its hidden state and process the concatenated input, outputing a further predicted word. The whole process is carried out until the decoder outputs EOS.\n",
    "\n",
    "## Visualizations of attention\n",
    "\n",
    "The attention weights can be plotted as a matrix, which gives a visual of how does the attention span evolve through the sentence.\n",
    "\n",
    "## Other applications:\n",
    "\n",
    "This has also been succesfully applied for image captioning (Xu et al 2015 *Neural image caption generation with visual attention*)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Speech Recognition\n",
    "\n",
    "* Discussion about preprocessing: STFT, phoneme model...\n",
    "* Data size: In academia 300, 3000h are not uncommon, but top commercial systems take 100k to 1 million hours.\n",
    "* Discusion about resolution: audio sequences have way bigger frequency than token-based sequences\n",
    "\n",
    "* Attention model would work here\n",
    "\n",
    "## CTC cost for speech recognition\n",
    "\n",
    "(Connectionist temporal classification, Graves et al 2006).\n",
    "\n",
    "* The idea is that a symbol can be identified in many repeated steps, and between symbols they may be unknown 'blank' assigned characters (which are different from 'space' characters), so **repeated symbols not separated by a blank are collapsed into one**:  `__c_oo_o_kk___b_ooooo__oo__kkk -> cookbook`\n",
    "\n",
    "Check the paper"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Trigger Word Detection\n",
    "\n",
    "We know that much that this can be explained in one slide: This is like entity recognition, and our entity is a trigger word: for each input (which is some feature such as an STFT frame) we predict whether we just finished a trigger word (1) or not (0).\n",
    "\n",
    "To balance data, instead of a single 1 a row of several 1s can be set. This is a hack but works"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Assignments"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
